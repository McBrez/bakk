%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
%   File:     metholodgy.tex                                                   %
%   Document: XXX	                                                           %
%   Author:   Freismuth David                                                  %
%	Date:	  22.JUN.2018                                                      %
%   Content:  Contains the Metholodgy section of the Bachelor thesis.          %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

To achieve our goal of the categorization of hardware designs trough a structural analysis of the corresponding HDL Design, we firstly propose a non-standardized match vector, which contains the count of the Design's Two-to-One and One-to-One Gate Level Connections (see \label{logicGates} referencce) as each of its entries, and secondly a standardized match vector, which reduces the information hold by the non-standardized match vector down to a two dimensional vector. This standardized match vector shall then act as categorization criteria, to sort the design into predefined design categories. Those categories are clusters in the vector space the standardized match vector is part of. If the match vector points into such a cluster, the design is identified with the corresponding category.
This section elaborates on the details of finding a match vector of a design, and
how the category clusters are defined. 

\subsection{Cluster Identification}
As mentionend above, before any categorization can take place, the categories itself
have to be defined. In our model, categories are clusters in a two dimensional vector space. 
Therefore, the task is to identify clusters in this vector space, and name them. 
To do this, we determine the match vectors of well known designs. Since
the categorization of those designs is already available, the vectors that originate from designs with identical categorizations, can be grouped together to clusters. 

It is expected, that the validity of such clusters increases with the number of analyzed designs. Therefore an automatic process has to be established, to enable the analysis of a great amount of designs. Conveniently, the website \gls{oc} offers a great collection of Hardware Designs that already have been categorised manually. \gls{oc} though does not offer an interface to automatically download all available design files. Therefore it has to be resorted to the web page, which is optimized for human interaction, but not for machine readability. Luckily there are Tools, which are spezialized in translating human readable content into structured, machine oriented data sets. These are called \glspl{scraper}. \gls{scrapy} \autocite{scrapy} is a python framework that exposes the functionallities of such a \gls{scraper}, and provides multiple python classes which enable extensible web scraping. Because of its ease of use and extensibility, \gls{scrapy} has been chosen, to solve the majority of problems that come with the challenge of retrieving a great amount of Hardware Designs.

Among the set of information that is specifically provided for each design by \gls{oc}, we fetch and process following information.  

\begin{enumerate}

	\item{\gls{url} to the design archive}
       
       So the Cluster Identification can be validated at a later point in time (asuming the files still exist under the same link).

	\item{The name of the design project}
       
       Used to address single designs. 

	\item{The \gls{hdl} in which the design is specified and implemented}
       
       Necesarry for the synthesis process, later in the cluster identification process

	\item{The category in which a design is listed}
       
       Mandatory for the Cluster identification process. 
        
	\item{The \gls{hdl} files of the design}
       
       Trivially necesarry to analyse the hardware design.
        
\end{enumerate}

Scrapy is able to process data within a pipeline structure, which, in our case, enables us to push user defined data sets trough those stages, an perform specific actions on them. The advantage herein lies in the parallelity, and therefore speed increase, that can be achieved by using this strucutre, since multiple pipelines can be active at a given time. \cref{fig:scrapyPipeline} shows the pipeline stages of the scrapy implementation that has been developed during the run of this thesis.
The following chapters explain in detail, how those Pipeline stages manipulate the data stated in the enumeration above.

\begin{figure}
    \centering
    \begin{tikzpicture}[scale = 1]
        \tikzstyle{every node} = [scale = 1, rectangle, fill=gray!30, minimum width = 6em, minimum height = 1em)]
        \node(Pipeline 1)[fill = white] at (0, 0) {Pipeline 1};
        \node(Fetch1) at (0, -1){Fetch};
        \node(Download1) at (0, -2) {Download};
        \node(Decompress1) at (0, -3) {Decompress};
        \node(Synthesize1) at (0, -4) {Synthesize};
        \node(Find pass1) at (0, -5) {Find Pass};
        \draw [->] (Fetch1) -- (Download1);
        \draw [->] (Download1) -- (Decompress1);
        \draw [->] (Decompress1) -- (Synthesize1);
        \draw [->] (Synthesize1) -- (Find pass1);
        
        \node(Pipeline 2)[fill = white] at (3, 0) {Pipeline 2};
        \node(Fetch2) at (3, -1){Fetch};
        \node(Download2) at (3, -2) {Download};
        \node(Decompress2) at (3, -3) {Decompress};
        \node(Synthesize2) at (3, -4) {Synthesize};
        \node(Find pass2) at (3, -5) {Find Pass};
        \draw [->] (Fetch2) -- (Download2);
        \draw [->] (Download2) -- (Decompress2);
        \draw [->] (Decompress2) -- (Synthesize2);
        \draw [->] (Synthesize2) -- (Find pass2);
        
        \node(ellipsis)[fill = none, scale = 2] at (5,-3) {...};
        
        \node(Pipeline n)[fill = white] at (7, 0) {Pipeline n};
        \node(Fetchn) at (7, -1){Fetch};
        \node(Downloadn) at (7, -2) {Download};
        \node(Decompressn) at (7, -3) {Decompress};
        \node(Synthesizen) at (7, -4) {Synthesize};
        \node(Find passn) at (7, -5) {Find Pass};
        \draw [->] (Fetchn) -- (Downloadn);
        \draw [->] (Downloadn) -- (Decompressn);
        \draw [->] (Decompressn) -- (Synthesizen);
        \draw [->] (Synthesizen) -- (Find passn);
        
        \node(Clustering) at (3, -6) {Clustering};
        \draw [->] (Find pass1) -- (Clustering);
        \draw [->] (Find pass2) -- (Clustering);
        \draw [->] (Find passn) -- (Clustering);
        

    \end{tikzpicture}
    \label{fig:scrapyPipeline}   
    \caption{ Pipeline stages of the \gls{scrapy} implementaion. As seen in the figure, Pipelines can be executed parallelly. The final stage "Clustering" is not implemented as pipeline stage, but rather waits on the completion of all pipelines to process all the gathered data. }
\end{figure}

\subsubsection{Fetch} 
In this stage, the \gls{oc} site is called to find the \glspl{url} where the hardware designs and corresponding meta informaiton resides. As \gls{oc}'s web page is built, like most web pages, in a tree like structure (see \cref{fig:opencoresStructure} for reference), \gls{scrapy} has been configured to start from the root page www.opencores.org, where the links to all projects reside (corresponds to the function call in \cref{lst:scrapyMainClass} line >>XXXX<<). "Start" in this context means, that the \Gls{html} representation of the site is retrieved by \gls{scrapy} and exposed via a \Gls{python} class to the User. \Gls{scrapy} then offers functions to extract informations from the \gls{html} code, like filtering for all <h> elements (which represent \glspl{hyperlink} that fork deeper into the website.).

\begin{figure}
 \centering
 \begin{tikzpicture}[grow via three points={one child at (0.5,-0.7) and two children at (0.5,-0.7) and (0.5,-1.4)},
  edge from parent path={(\tikzparentnode.south) |- (\tikzchildnode.west)}]
  \tikzstyle{every node}=[draw=black,thick,anchor=west]
  \tikzstyle{selected}=[draw=red,fill=red!30]
  \tikzstyle{optional}=[dashed,fill=gray!50]
  \node {opencores.org}
    child { node {opencores.org/projects}
        child{ node [selected] {opencores.org/project/audio}}
        child{ node [selected] {opencores.org/project/adder_subtractor}}
        child{ node [selected] {opencores.org/project/...}}
    }
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child { node {opencores.org/forum}}
    child { node {opencores.org/licensing}}
    child { node {opencores.org/...}};
 \end{tikzpicture}
 \caption{The structure of the \gls{oc} website. The \Glspl{url} where the projects are located, are marked red.}
 \label{fig:opencoresStructure}
\end{figure}


As seen in \cref{lst:scrapyMainClass} line >>XXXXX<< for each >>element<<< that has been found, a new function >>>function<<< is invoked, which further calls the webpage the >>elements<< points to. 

\begin{lstlisting}[style = python, caption = {Scrapy Main Class}, label = {lst:scrapyMainClass}]
 class IPspider_oc(scrapy.Spider):
    """The class, which is called by the scrapy framework. """
    name = 'ipspider_oc'   # the name of the spider

    def start_requests(self):
        return [scrapy.Request(
          url = "https://opencores.org",
          callback=self.login)]

    def login(self, response):
        return scrapy.FormRequest.from_response( 
          response, 
          formdata={'user' : 'davFreismuth',  
                    'pass' : 'Qlghkeul'}, 
          callback=self.redirect )

    def redirect(self,  response):
        return scrapy.Request( 
          url = "https://opencores.org/projects?lang=0&stage=5&license=0&wishbone_version=0", 
          callback=self.parse )
                                                
    def parse(self, response):
        """Default callback for scrapy. Starts at start_url, fetches the url of the different project pages, and calls parse_metadata for each project page"""
        for href in response.css('td.project a::attr(href)'):
            yield response.follow(href, self.parse_metadata)

    def parse_metadata(self, response):
        """parses metadata of an opencore.org project page and returns a scrapy item object."""
        def scrape_line(strArray,  removeStr):  
            '''helper function which iterates trough the string array 'strArray', selects the first entry which contains 'removeStr' and returns the string without removeStr and whitespaces.'''
            for str in strArray:
                if(str.find(removeStr) != -1):
                    retStr = str.replace(removeStr,  '')
                    return retStr.strip()
            
        def scrape_href(line):  
            """helper function, which gets an html <a> element, checks if it has inner html and returns the text of the inner html"""
            start = line.find(">")
            end = line.find("<", start)
            if((start + 1) == end):
                return ""
            else:
                return line[(start+1):(end)]
            
        #fill up the fields
        hdl_IP =  HDL_IP()
        hdl_IP['name'] = scrape_line(response.css('h2 + p::text').extract(), 'Name:')
        hdl_IP['created'] = scrape_line(response.css('h2 + p::text').extract(), 'Created:')
        hdl_IP['updated'] = scrape_line(response.css('h2 + p::text').extract(), 'Updated:')
        hdl_IP['file_urls'] = ['https://opencores.org' + response.css('p a::attr(href)')[1].extract()]
        category = scrape_href(response.css('p a')[5].extract())
        category = category.lower()
        category = category.replace(' ', '_')
        hdl_IP['category'] = category
        hdl_IP['language'] = scrape_href(response.css('p a')[6].extract())
        hdl_IP['license'] = scrape_line(response.css('h2 + p::text').extract(),  'License:')
        hdl_IP['basePath'] = self.settings['FILES_STORE']
        yield hdl_IP
\end{lstlisting}

\paragraph*{Authentication \\}

Besides navigating to and trough the webpage, another task for the Fetch stage is the authentication with the \Gls{oc} user system, since designs are only downloadable, while logged in with a cost free \Gls{oc} account.

Login information is often communicated via \Gls{html} \Gls{post} Messages, which encode the message content into the message header. In the login case, the login information is encoded into the message header. After the login information has been accepted by the web page, it returns an authentication token, which is stored as \Gls{cookie} and enables access to the web page. Scrapy is natively able to generate and send such \Gls{post} messages and exposes the answer to such \Gls{post} messages in Python classes, for further actions. The function which sends the \Gls{post} message can be seen in \cref{lst:scrapyMainClass} line >>XXXXX<<.

\paragraph*{Extraction \\}
Once \Gls{scrapy} navigated to the project page, the above mentionend information is extracted by using \Gls{css} selectors, as seen in \cref{lst:scrapyMainClass} line >XXXXXX< to >>YYYYYYYY<<. The so gathered data sets are stored in the user defined \Gls{python} \lstinline{HDL_IP} object (derived from the \Gls{scrapy} class \lstinline{item}), which serves as container that gets pushed trough the Pipeline stages. The Fetch stage ends with returning the \lstinline{HDL_IP} object to the \Gls{scrapy} framework, where it gets handed over to the next Pipeline stage.  

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth,keepaspectratio]{fig/jsonFileFormatDescription.JPG}
	\caption{.JSON File Format Description}
	\label{fig:jsonFormat}
\end{figure}

Figure \Cref{fig:jsonFormat} shows an example of the content of the .JSON file. 

\subsubsection{Download}
The \Gls{scrapy} \Gls{python} module provides the functionallity to automatically download and
store files that are associated with a Hardware design project, to a predefined
folder. The download is started by pushing an \lstinline{item} object with an \lstinline{URL} atrribute into a pipeline stage declared as a donwload stage. The \Gls{scrapy} framework then begins the download of the file specified in the \lstinline{URL} atrribute to the path defined in the atrribute \lstinline{spider.settings['FILE_STORE']}. If the file has been downloaded successfully, its name gets added to the \lstinline{HDL_IP} object under the attribute \lstinline{item.file}.  

\subsubsection{Decompress} 
Projects from \gls{oc} are solely provided eiter as *.tar.gz or *.zip compressed archive. The stage 'Decompress' is therefore dedicated to decompress the previously downloaded archive and to order the contained files into an predefined structure. The \Gls{python} modules \lstinline{tarfile} and \lstinline{zipfile} enable the extraction of such archives.
After the decompression, the project files are sorted into folders corresponding to 
their associated hardware category (as stated by \gls{oc}). After that, the file 
endings of the files are analysed. If those endings indicate a \gls{hdl} file, the path 
to this file is added to the aforementioned \lstinline{HDL_IP} object, in order to be able to
address the design files of a project in later stages. If no such files could be found, the \lstinline{HDL_IP} object is discarded, and will not be further processed. 

\subsubsection{Synthesis} 
After the design has been decompressed, it is time to let the synthesis tool \Gls{yosys} read the design files and synthesise them into a text file format. In order to do this in an automated manner, for each discovered design, a yosys script file is generated on the basis of the information, that has been gathered in previous steps. This Script instructs \Gls{yosys} about the path to the discovered files and with which frontend to load them. A sample script file can be seen in \cref{lst:yosysScriptFile}.
An alternative for this automatic script generation is to supply a custom made script, which will then be loaded instead of the generic one. Bigger design may need to be synthesised with such a custom script, since the dependecies may be much to complex, to be computed automatically. 

 \begin{lstlisting}[style = python, caption = {An example Yosys Script file.}, label = {lst:yosysScriptFile}]
  asdfsdafasdfasdfasdfsdaf
 \end{listing}
 \caption{An example Yosys Script file.}
 \label{lst:yosysScriptFile}
 \end{lstlisting}

The synthesis tool's frontend is chosen based on the language of each design file.
For \gls{vhdl} and SystemVerilog, the \lstinline{verific} frontend is used. For Verilog,
we use the \lstinline{read_verilog} frontend. Once any combination of \gls{hdl} files
has been loaded, a synthesize run attempts to generate a single HDL file from the provided
files, which solely contains primitive logic blocks. 

Since yosys does not support automatic dependency recognition of vhdl files, a custom solution
had to be found, to determine the load order of vhdl files (in the case that the user decides
that yosys scripts should be generated automatically). To accomplish this, we slightly modified 
the Vunit python project, which offers a function to return the \gls{vhdl} 
files in an ordered list. The vhdl files can then be loaded in the order dictated by this list.  

\subsubsection{Find Pass} 
Each design that is read by the synthesis tool is named after the project from
which the design files are downloaded. From then, the design name is the main
reference for each design and serves as identification feature in all
subsequent steps.

\subsubsection{Clustering}
blabla

\subsection{Design Identification}
blabla

\subsection{Verification}
blabla



